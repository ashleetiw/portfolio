<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<title>Deep Learning-Based Eye Gaze Estimation using Deflectometry Information in VR/AR/MR Headsets - Hi! I'm Ashlee Tiwari</title>

  <!-- Edit site and author settings in `_config.yml` to make the social details your own -->

    <meta content="Hi! I'm Ashlee Tiwari" property="og:site_name">
  
    <meta content="Deep Learning-Based Eye Gaze Estimation using Deflectometry Information in VR/AR/MR Headsets" property="og:title">
  
  
    <meta content="article" property="og:type">
  
  
    <meta content="The goal of the project is to enable faster and more precise eye gaze direction estimation in VR/AR/MR devices by exploiting the deflectometry information provided from the reflection of the screen pattern on the specular surface of the eye" property="og:description">
  
  
    <meta content="/ashleetiw/kernel/" property="og:url">
  
  
    <meta content="2022-03-05T00:00:00-06:00" property="article:published_time">
    <meta content="/ashleetiw/about/" property="article:author">
  
  
    <meta content="/ashleetiw/portfolio/assets/img/eye-tracking-in-vr.jpg" property="og:image">
  
  
    
  
  
    
    <meta content="Eye tracking" property="article:tag">
    
    <meta content="Pattern Optimization" property="article:tag">
    
    <meta content="Deep Learning" property="article:tag">
    
  

    <meta name="twitter:card" content="summary">
    <meta name="twitter:site" content="@">
    <meta name="twitter:creator" content="@">
  
    <meta name="twitter:title" content="Deep Learning-Based Eye Gaze Estimation using Deflectometry Information in VR/AR/MR Headsets">
  
  
    <meta name="twitter:url" content="/ashleetiw/kernel/">
  
  
    <meta name="twitter:description" content="The goal of the project is to enable faster and more precise eye gaze direction estimation in VR/AR/MR devices by exploiting the deflectometry information provided from the reflection of the screen pattern on the specular surface of the eye">
  
  
    <meta name="twitter:image:src" content="/ashleetiw/portfolio/assets/img/eye-tracking-in-vr.jpg">
  

	<meta name="description" content="The goal of the project is to enable faster and more precise eye gaze direction estimation in VR/AR/MR devices by exploiting the deflectometry information provided from the reflection of the screen pattern on the specular surface of the eye">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
	<meta property="og:image" content="">
	<link rel="shortcut icon" href="/portfolio/assets/img/favicon/favicon.ico" type="image/x-icon">
	<!-- <link rel="apple-touch-icon" href="/portfolio/assets/img/favicon/apple-touch-icon.png">
	<link rel="apple-touch-icon" sizes="72x72" href="/portfolio/assets/img/favicon/apple-touch-icon-72x72.png">
	<link rel="apple-touch-icon" sizes="144x144" href="/portfolio/assets/img/favicon/apple-touch-icon-144x144.png"> -->
	<!-- Chrome, Firefox OS and Opera -->
	<meta name="theme-color" content="#263959">
	<!-- Windows Phone -->
	<meta name="msapplication-navbutton-color" content="#263959">
	<!-- iOS Safari -->
	<meta name="apple-mobile-web-app-status-bar-style" content="#263959">
	<!-- Google Fonts -->
	<link href="https://fonts.googleapis.com/css?family=PT+Serif:400,700|Lato:300,400,700&display=swap" rel="stylesheet">
	<!-- Font Awesome -->
	<link rel="stylesheet" href="/portfolio/assets/fonts/font-awesome/css/font-awesome.min.css">
	<!-- Styles -->
	<link rel="stylesheet" href="/portfolio/assets/css/main.css">
</head>

<body>

  <div class="wrapper">
    <aside class="sidebar">
  <header>
    <div class="about">
      <div class="cover-author-image">
        <a href="/portfolio/"><img src="/portfolio/assets/img/me.png" alt="Ashlee Tiwari"></a>
      </div>
      <div class="author-name">Ashlee Tiwari</div>
      <p>I am eager to learn new domains and enjoy building solutions to challenging problems. Education:Northwestern University | IIT Kanpur</p>
    </div>
  </header> <!-- End Header -->
  <footer>
    <section class="contact">
      <h3 class="contact-title">Contact me</h3>
      <ul>
        <!-- 
          <li><a href="https://twitter.com/artemsheludko_" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a></li>
        
        
          <li><a href="https://facebook.com/" target="_blank"><i class="fa fa-facebook" aria-hidden="true"></i></a></li>
         -->
        
          <li class="github"><a href="http://github.com/ashleetiw" target="_blank"><i class="fa fa-github"></i></a></li>
        
        
          <li class="linkedin"><a href="https://in.linkedin.com/in/ashleetiwari" target="_blank"><i class="fa fa-linkedin"></i></a></li>
        
        
          <li class="email"><a href="mailto:ashleetiwari2021@u.northwestern.edu"><i class="fa fa-envelope-o"></i></a></li>
        
      </ul>
    </section> <!-- End Section Contact -->
    <div class="copyright">
      <p>2022 &copy; Ashlee Tiwari</p>
    </div>
  </footer> <!-- End Footer -->
</aside> <!-- End Sidebar -->
<div class="content-box clearfix">
  <article class="article-page">
  <div class="page-content">
    
    <div class="page-cover-image">
      <figure>
        <img class="page-image" src=/portfolio/assets/img/eye-tracking-in-vr.jpg alt="Deep Learning-Based Eye Gaze Estimation using Deflectometry Information in VR/AR/MR Headsets">
        
      </figure>
    </div> <!-- End Page Cover Image -->
    
    <div class="wrap-content">
      <header class="header-page">
        <h1 class="page-title">Deep Learning-Based Eye Gaze Estimation using Deflectometry Information in VR/AR/MR Headsets</h1>
        <div class="page-date"><span>March-June 2021&nbsp;&nbsp;&nbsp;&nbsp;</span></div>
      </header>
      <p>The goal of the project is to enable faster and more precise eye gaze direction estimation in VR/AR/MR devices by exploiting the deflectometry information provided from the reflection of the screen pattern on the specular surface of the eye.</p>

<p>This is my final graduate project at Northwestern University. I was supervised by</p>
<ol>
  <li>Professor Florian Willomitzer, and Oliver Cossairt, ECE/CS Department</li>
  <li>Nathan Matsuda, Ph.D., Facebook Reality Labs</li>
  <li>Professor Matthew Elwin, Deputy Director of the Master of Robotics Program</li>
</ol>

<h2 id="motivation">Motivation</h2>
<p>The project aims to solve one of the current problems in Virtual, Augmented, or Mixed Reality
Headsets: Accurate and fast eye tracking. Precise eye tracking in VR/AR/MR Headsets can help to significantly increase the viewing comfort by continuously keeping track of the inter-pupillary distance (IPD) of the viewer. This is important since changing the eye’s focus (accommodation) is in natural scenes directly connected to the vergence of the optical axes of both eyes(“accommodation-convergence reflex”).</p>

<h2 id="problem-statement">Problem Statement:</h2>
<p>High-quality gaze tracking requires sufficient 3D scene understanding since the vertex of the eye must be precisely tracked
in three dimensions.This requires a precise and dense single-shot 3D measurement of the eye surface and a procedure to evaluate
these measurement to calculate the gazing direction.</p>

<h2 id="the-principle-of-the-normal-measurement-in-deflectometry">The principle of the normal measurement in deflectometry</h2>
<p>The purpose of this project is to significantly increase the information content that is provided by corneal or scleral reflection to calculate the gazing direction.</p>

<p><img src="../assets/img/deflectometry.jpg" alt="approach" width="600" /></p>

<p>The basic principle of Deflectometry: A screen that displays a known pattern replaces the point-like light source.
In Deflectometry systems, the screen and camera face the object, which means that the camera observes the specular reflection of the screen over
the object’s surface. The observed pattern in the camera image is a deformed version of the image on the screen,
where the deformation depends on the surface normal distribution of the object surface. From this deformation,
the normal vectors of the surface can be calculated.</p>

<h2 id="model-design-swirski-eye">Model Design: Swirski Eye</h2>
<p>It is very crucial to have a model that captures realistic face geometry, including eyelids and eyebrows, that animates in response to the eye movements and different closeness settings. To create a realistic render of the eye, I used the model Swirski and Dodgson developed in their paper. Since
the original Swirski model did not use an eyeball with the elevated cornea, I swapped the eyeball in the Swirski model and replaced it with my own eye model while keeping the face model.
To load and animate the model I used Blender, which is a free and open-source 3D computer graphics software.</p>

<p><img src="../assets/img/eyemodel.gif" alt="eye" width="600" /></p>

<!-- ## Single Frame Deflectometry Network(SFDN) vs Double Frame Deflectometry (DFDN)
The project initially used SFDN which takes a single eye image as an input and predicts two rotation angles of the eye: azimuth and elevation
Drawback: SFDN can only work well with a fixed pattern that it was trained with.

To overcome this issue, I chose Double Frame Deflectometry Network(DFDN), to allow arbitrary screen patterns for the inputs


DFDN takes a pair of eye images, where one is an actual captured image and the other
is a synthesized eye image with preset rotation angles and the reflection of an arbitrary pattern(reference image)

A reference image plays a role in providing the network
with some information about the arbitrary pattern that is being reflected on the captured eye image. -->

<h2 id="architecture-of-the-double-frame-deflectometry-network">Architecture of the Double Frame Deflectometry Network</h2>

<p>Goal: To learn the relationship between the screen pattern of a captured image and that of a reference image to estimate the gaze direction based on the deformation of the pattern due to the rotation of the eye.</p>

<p><img src="../assets/img/model.png" alt="model" width="600" /></p>

<p>I chose Double Frame Deflectometry Network(DFDN), to allow arbitrary screen patterns for the inputs. DFDN takes a pair of eye images, where one is an actual captured imag and the other is a synthesized eye image with preset rotation angles and the reflection of an arbitrary pattern(reference image). A reference image plays a role in providing the network with some information about the arbitrary pattern that is being reflected on the captured eye image.</p>

<h2 id="investigation-of-effects-of-various-patterns-on-gaze-estimation-accuracy">Investigation of effects of various patterns on gaze estimation accuracy</h2>

<h3 id="random-pattern-with-various-low-pass-filters">Random pattern with various low pass filters</h3>
<p><img src="../assets/img/gaussian.png" alt="g" width="600" /></p>

<h3 id="sinusoids-of-different-periods">Sinusoids of different periods</h3>
<p><img src="../assets/img/sine.png" alt="sine" width="600" /></p>

<h3 id="effect-of-aliasing">Effect of Aliasing</h3>
<p>These results are counter-intuitive and  we hypothesize that this nonlinearity between frequency and accuracy may be due to a severe undersampling happening at the boundary of the cornea and sclera because of the abrupt change in curvature.</p>

<p><img src="../assets/img/alias_70.png" alt="alias" width="400" /></p>

<h2 id="pattern-optimization">Pattern optimization</h2>
<p>We concluded the following:</p>
<ol>
  <li>The true relationship between the frequency of the pattern and the estimation accuracy may be nonlinear. There may be a sweet spot, or the ideal frequency, where the network can minimize the estimation error. (This is due to the presence of a sharp edge between cornea and sclera)</li>
  <li>The frequency might not be the exact property that is associated with accuracy.</li>
</ol>

<p>The test various hypothesis on the properties of patterns that may affect the accuracy, and in the process we aim to find the optimal
the pattern that yields the lowest error.
In this process I focused on using algorithms to find the optimal pattern in two steps:</p>
<ol>
  <li>To perform a course-level optimization to determine the shape of the function. This was achieved by using the genetic algorithm (which is mentioned in a different post) which eliminated some of the functions from the pool of hypothetical functions formed using a fitness function.</li>
  <li>To perform a fine-level optimization to fine-tune the constants. This was achieved by gradient descent optimization on L2 loss which was calculated on gazing direction values( azimuthal and elevation angle).
<img src="../assets/img/final.gif" alt="approach" width="600" />
<img src="../assets/img/my-eye.png" alt="my-eye" width="190" /></li>
</ol>


      <div class="page-footer">
        <div class="page-share">
        </div>
        <div class="page-tag">
          
            <a href="/portfolio/tags#Eye tracking" class="tag">&#35; Eye tracking</a>
          
            <a href="/portfolio/tags#Pattern Optimization" class="tag">&#35; Pattern Optimization</a>
          
            <a href="/portfolio/tags#Deep Learning" class="tag">&#35; Deep Learning</a>
          
        </div>
      </div>
      <!-- <section class="comment-area">
  <div class="comment-wrapper">
    
  </div>
</section> <!-- End Comment Area -->
 -->
    </div> <!-- End Wrap Content -->
  </div> <!-- End Page Content -->
</article> <!-- End Article Page -->

</div>

  </div>
  
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', '', 'auto');
  ga('send', 'pageview');
</script> 
<!-- End Analytics -->

</body>
</html>
